import base64import urllib3from bs4 import BeautifulSoupfrom flask import Blueprint, requestfrom werkzeug.exceptions import BadRequestfrom application.utils.decorators import json_responsebp_articles = Blueprint("bp_articles", __name__)@bp_articles.route("/", methods=["GET"])@json_responsedef get_articles():    query = request.args.get("query")    next_page_token = request.args.get("next_page_token")        if not query:        raise BadRequest    result = {"status": "ok"}    current_page = 1    if not next_page_token:        current_page = int(str(base64.b64decode(next_page_token)                               .decode("utf-8", "ignore")).split("-")[1])    http = urllib3.PoolManager()    response = http.request("GET", "https://libgen.is/scimag/?q="                             + query + "&page=" + str(current_page))    html_text = response.data    soup = BeautifulSoup(html_text, "html.parser")    try:        total_value = str(soup.find("div", attrs={"style": "float:left"})                          .getText()).split(" ")[0]    except Exception as e:        result["data"] = []        result["detail"] = str(e)        result["status"] = "failure"        return result    total_page_dobule = int(total_value) / 25    total_page = int(int(total_value) / 25)    next_page = None    if total_page != total_page_dobule:        total_page += 1    if current_page < total_page:        next_page = current_page+1    if next_page is not None:        next_page_token = base64.b64encode(("univerdustry-"+str(next_page))                                           .encode("utf-8")).decode("utf-8")        result["next_page_token"] = next_page_token    counter = 0    articles = []    for link in soup.find_all("tr"):        if counter == 0:            counter += 1            continue        item = link.find_all("td")        author = str(item[0]).replace("<td>", "").replace("</td>", "")\            .split(";")        url_for_get = item[4].find_all("li")        href = url_for_get[1].find_all("a", href=True)[0]["href"]        response_for_pdf = http.request("GET", href)        pdf_page = BeautifulSoup(response_for_pdf.data, "html.parser")        pdf_url = pdf_page.find_all(            "td", {"align": "center"}        )[0].find_all("a", href=True)[0]["href"]        year_index = str(response_for_pdf.data).find("Year: ")        year = str(response_for_pdf.data)[year_index + 6: year_index + 10]        title = item[1].find_all("a")[0].text        info = {            "author": author,            "year": year,            "url": pdf_url,            "title": title        }        articles.append(info)    result["data"] = articles    return result
import jsonimport requests as reqfrom application import celery, logger, esfrom application.utils.helpers import get_configfrom application.utils.text import preprocess_textfrom application.rests.vectorizer import get_vectorfrom application.rests.elasticsearch import get_docsfrom application.rests.mongo import find, count_documnetsfrom application.utils.mappings import publication_mappingsfrom application.utils.decorators import celery_exception_handlerfrom application.tasks.elasticsearch_indexing import t_elasticsearch_indexing@celery.task(bind=True, name="update_vectors", max_retries=3)@celery_exception_handler(ConnectionError)def t_update_vectors(self):    resd = {"status": "ok"}    count = count_documnets("publication")    logger.info(f'Starting Vector Update Task!')    logger.info(f'Document Count: {count}')    langs = get_config("LANGUAGES")    for lang in langs:        publication_mappings["properties"][f'title_{lang}'] = {"type": "text"}        publication_mappings["properties"][f'content_{lang}'] = {"type": "text"}    resp = req.put(        get_config("ELASTICSEARCH") + "/publication",        json={"mappings": publication_mappings}    )    if resp.status_code == 400:        resp = req.put(            get_config("ELASTICSEARCH") + "/publication/_mappings",            json=publication_mappings        )    logger.info(f'Mapping Response: {resp.json()}')    with open("pubs_es.json", "rt", encoding="utf-8") as fp:        pubs = json.load(fp)    for pub in pubs:        try:            _id = pub["_id"]            pub = pub["_source"]            lang = pub["lang"]            if pub.get("vector", None) is None:                vector_field_tokens = list()                if pub.get(f'content_{lang}', None):                    vector_field_tokens += pub[f'content_{lang}'].split()                if not pub[f'title_{lang}'].startswith("unk_"):                    vector_field_tokens += pub[f'title_{lang}'].split()                vector_field = " ".join(vector_field_tokens)                vectorizer_response = get_vector(preprocess_text(vector_field))                pub["vector"] = vectorizer_response["vector"]            es.index(index="publication", body=pub, id=_id)        except Exception as e:            logger.exception(e)    return resd# for i in range(10, 21, 10):#     logger.info(f'Processed: {i}/{count}')##     pubs = find("publication", {#         "projection": ["id", "content", "title"], "skip": i,#         "limit": 10#     })##     for pub in pubs:#         vector_field_tokens = list()#         if pub.get("content", None):#             vector_field_tokens += pub["content"].split()#         if not pub["title"].startswith("unk_"):#             vector_field_tokens += pub["title"].split()##         vector_field = " ".join(vector_field_tokens)#         vectorizer_response = get_vector(preprocess_text(vector_field))#         vector = vectorizer_response["vector"]##         logger.info(f'Pub({pub["id"]})')##         pub_in_es = get_docs([pub["id"]], ["id"])##         logger.info(f'pes: {pub_in_es}')##         if len(pub_in_es) == 1:#             es.index(index="publication",#                      body={"vector": vector},#                      id=pub["id"])#         else:#             t_elasticsearch_indexing.apply_async((pub["id"],))